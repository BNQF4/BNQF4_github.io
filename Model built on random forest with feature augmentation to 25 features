# Model built on random forest with feature augmentation to 25 features 

# Import necessary libraries
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import learning_curve 
# Load the dataset
df = pd.read_excel('D:/Users/admin/Desktop/Original Dataset.xlsx')
# Check for NaN values in each column
nan_columns = df.columns[df.isnull().any()].tolist()
print("Columns containing NaN values:")
print(nan_columns)
# A few columns that I didn't find useful have been deleted
data = df.drop(['MaterialFormula','Rate Capability', 'c rate', 'MO6 Octahedra', 'MO4 Tetrahedra', 'Calcination Temp',
       'Synthesize Time', 'Starting Materials', 'Reaction Atmosphere',
       'Preparation Method', 'Min Working Potential', 'Max Working Potential',
       'Testing Temp', 'Reference'], axis = 1).copy()
# Check for NaN values in each column
nan_columns = data.columns[data.isnull().any()].tolist()

print("Columns containing NaN values:")
print(nan_columns)
#Fill up the NaN in numeric columns with 0
columns_to_fill_with_0 = ['Dopant Ratio', 'Nb Ratio', 'O Ratio',
       'Molecular Weight', 'Initial discharge capacity',
       'Initial charge capacity', 'First Cycle Coulombic efficiency', 'Capacity Retention',
       'Cycles', 'C rate', 'Theoretical Capacity', 'Current Density', 'a(Å)', 'b(Å)',
       'c(Å)', 'UnitCellVolume',
       ' Li-ion Diffusion Coefficient on discharging (Lithiation)',
       'Li-ion Diffusion Coefficient on charging (Delithiation)',
       ' Electronic Conductivity (S cm-1)',
       'BET specific surface area (m^2 g-1)']

# Fill NaN values in specified columns with 0
data[columns_to_fill_with_0] = data[columns_to_fill_with_0].fillna(0)
#Fill up the NaN values with the most frequently occurring ones in categorical features
# Columns to fill NaN values with the most frequent value
columns_to_fill_with_most_frequent = ['Microstructure', 'Space Group']
# Fill NaN values in specified columns with the most frequent value
for column in columns_to_fill_with_most_frequent:
    most_frequent_value = data[column].mode().iloc[0]
    data[column] = data[column].fillna(most_frequent_value)
# Save this processed dataset as preprocessed_stage2_dataset
save_location = ' D:/Users/admin/Desktop/preprocessed_stage2_dataset.xlsx'
data.to_excel(save_location, index=False)
# Load the new dataset
data_new = pd.read_excel(' D:/Users/admin/Desktop/preprocessed_stage2_dataset.xlsx ')
# Assuming 'Capacity Retention' is the target variable, and other columns are features
X = data_new.drop(columns=['Capacity Retention'])
y = data_new['Capacity Retention']
# Perform one-hot encoding for categorical variables
X_encoded = pd.get_dummies(X, columns=['Dopant', 'Microstructure', 'Space Group'])
# Split the encoded data into training and testing sets
X_train_encoded, X_test_encoded, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=2205634)
# Create and train the Decision Tree model on the encoded data
tree_model = RandomForestRegressor(random_state=42)
tree_model.fit(X_train_encoded, y_train)
# Create and train the Random Forest model on the encoded data
forest_model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust the number of estimators as needed
forest_model.fit(X_train_encoded, y_train)
# Make predictions on the training set
y_train_pred_forest = forest_model.predict(X_train_encoded)
mse_train_forest = mean_squared_error(y_train, y_train_pred_forest)
r2_train_forest = r2_score(y_train, y_train_pred_forest)
# Make predictions on the test set
y_test_pred_forest = forest_model.predict(X_test_encoded)
mse_test_forest = mean_squared_error(y_test, y_test_pred_forest)
r2_test_forest = r2_score(y_test, y_test_pred_forest)
print("Random Forest - Training Mean Squared Error:", mse_train_forest)
print("Random Forest - Training R-squared:", r2_train_forest)
print("Random Forest - Test Mean Squared Error:", mse_test_forest)
print("Random Forest - Test R-squared:", r2_test_forest)
# Calculate R-squared values
r2_values = [r2_train_forest, r2_test_forest]
data_labels = ['Training', 'Test']
# Plot bar chart with green color for the test set
plt.bar(data_labels, r2_values, color=['blue', 'green'])
plt.ylim(0, 1)  # Set y-axis limit from 0 to 1 (R-squared ranges from 0 to 1)
plt.xlabel('Dataset')
plt.ylabel('R-squared')
plt.title('Random Forest Model - R-squared on Training and Test Sets')
plt.show()
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import learning_curve

# Create the learning curve for the Random Forest model
train_sizes, train_scores, test_scores = learning_curve(
    forest_model, X_train_encoded, y_train, cv=5, scoring='r2', n_jobs=-1)
# Calculate mean and standard deviation for training and test scores
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)
# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training R-squared')
plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color='r')
plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Test R-squared')
plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color='g')
plt.xlabel("Training Set Size")
plt.ylabel("R-squared")
plt.title("Random Forest Model - Learning Curve")
plt.legend(loc='best')
plt.grid(True)
plt.show()
